{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(722471, 10)\n",
      "(715884, 10)\n",
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/home/hadrien/Documents/Phd/ensemble-mia/data/tabddpm_black_box/train/tabddpm_203\",\n",
      "        \"exp_name\": \"train_1\",\n",
      "        \"workspace_dir\": \"/home/hadrien/Documents/Phd/ensemble-mia/data/tabddpm_black_box/train/tabddpm_203\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n",
      "Table name: trans, Total dataframe shape: (20000, 8), Numerical data shape: (20000, 4), Categorical data shape: (20000, 4)\n",
      "\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "['trans']\n",
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "\n",
      "Clustering checkpoint found, loading...\n",
      "==================== We show the important sampling parameters below ====================\n",
      "d_layers: [512, 1024, 1024, 1024, 1024, 512]\n",
      "dropout: 0.0\n",
      "num_timesteps: 2000\n",
      "model_type: mlp\n",
      "iterations: 200000\n",
      "batch_size: 4096\n",
      "lr: 0.0006\n",
      "gaussian_loss_type: mse\n",
      "weight_decay: 1e-05\n",
      "scheduler: cosine\n",
      "\n",
      "Training None -> trans model from scratch\n",
      "Model params: {'num_classes': 0, 'is_y_cond': 'none', 'rtdl_params': {'d_layers': [512, 1024, 1024, 1024, 1024, 512], 'dropout': 0.0}, 'd_in': 8}\n",
      "mlp\n",
      "Step 500/200000 MLoss: 0.0 GLoss: 0.2566 Sum: 0.2566\n",
      "Step 1000/200000 MLoss: 0.0 GLoss: 0.2393 Sum: 0.2393\n",
      "Step 1500/200000 MLoss: 0.0 GLoss: 0.2347 Sum: 0.2347\n",
      "Step 2000/200000 MLoss: 0.0 GLoss: 0.2307 Sum: 0.2307\n",
      "Step 2500/200000 MLoss: 0.0 GLoss: 0.2293 Sum: 0.2293\n",
      "Step 3000/200000 MLoss: 0.0 GLoss: 0.2274 Sum: 0.2274\n",
      "Step 3500/200000 MLoss: 0.0 GLoss: 0.2264 Sum: 0.2264\n",
      "Step 4000/200000 MLoss: 0.0 GLoss: 0.2259 Sum: 0.2259\n",
      "Step 4500/200000 MLoss: 0.0 GLoss: 0.2253 Sum: 0.2253\n",
      "Step 5000/200000 MLoss: 0.0 GLoss: 0.2252 Sum: 0.2252\n",
      "Step 5500/200000 MLoss: 0.0 GLoss: 0.2246 Sum: 0.2246\n",
      "Step 6000/200000 MLoss: 0.0 GLoss: 0.224 Sum: 0.224\n",
      "Step 6500/200000 MLoss: 0.0 GLoss: 0.2234 Sum: 0.2234\n",
      "Step 7000/200000 MLoss: 0.0 GLoss: 0.223 Sum: 0.223\n",
      "Step 7500/200000 MLoss: 0.0 GLoss: 0.2223 Sum: 0.2223\n",
      "Step 8000/200000 MLoss: 0.0 GLoss: 0.2228 Sum: 0.2228\n",
      "Step 8500/200000 MLoss: 0.0 GLoss: 0.2229 Sum: 0.2229\n",
      "Step 9000/200000 MLoss: 0.0 GLoss: 0.2218 Sum: 0.2218\n",
      "Step 9500/200000 MLoss: 0.0 GLoss: 0.2213 Sum: 0.2213\n",
      "Step 10000/200000 MLoss: 0.0 GLoss: 0.2212 Sum: 0.2212\n",
      "Step 10500/200000 MLoss: 0.0 GLoss: 0.2213 Sum: 0.2213\n",
      "Step 11000/200000 MLoss: 0.0 GLoss: 0.2207 Sum: 0.2207\n",
      "Step 11500/200000 MLoss: 0.0 GLoss: 0.2209 Sum: 0.2209\n",
      "Step 12000/200000 MLoss: 0.0 GLoss: 0.2204 Sum: 0.2204\n",
      "Step 12500/200000 MLoss: 0.0 GLoss: 0.2195 Sum: 0.2195\n",
      "Step 13000/200000 MLoss: 0.0 GLoss: 0.2195 Sum: 0.2195\n",
      "Step 13500/200000 MLoss: 0.0 GLoss: 0.2191 Sum: 0.2191\n",
      "Step 14000/200000 MLoss: 0.0 GLoss: 0.2195 Sum: 0.2195\n",
      "Step 14500/200000 MLoss: 0.0 GLoss: 0.22 Sum: 0.22\n",
      "Step 15000/200000 MLoss: 0.0 GLoss: 0.2196 Sum: 0.2196\n",
      "Step 15500/200000 MLoss: 0.0 GLoss: 0.2197 Sum: 0.2197\n",
      "Step 16000/200000 MLoss: 0.0 GLoss: 0.2187 Sum: 0.2187\n",
      "Step 16500/200000 MLoss: 0.0 GLoss: 0.2189 Sum: 0.2189\n",
      "Step 17000/200000 MLoss: 0.0 GLoss: 0.218 Sum: 0.218\n",
      "Step 17500/200000 MLoss: 0.0 GLoss: 0.2179 Sum: 0.2179\n",
      "Step 18000/200000 MLoss: 0.0 GLoss: 0.2187 Sum: 0.2187\n",
      "Step 18500/200000 MLoss: 0.0 GLoss: 0.2185 Sum: 0.2185\n",
      "Step 19000/200000 MLoss: 0.0 GLoss: 0.2177 Sum: 0.2177\n",
      "Step 19500/200000 MLoss: 0.0 GLoss: 0.2189 Sum: 0.2189\n",
      "Step 20000/200000 MLoss: 0.0 GLoss: 0.2179 Sum: 0.2179\n",
      "Step 20500/200000 MLoss: 0.0 GLoss: 0.2177 Sum: 0.2177\n",
      "Step 21000/200000 MLoss: 0.0 GLoss: 0.2169 Sum: 0.2169\n",
      "Step 21500/200000 MLoss: 0.0 GLoss: 0.2168 Sum: 0.2168\n",
      "Step 22000/200000 MLoss: 0.0 GLoss: 0.2172 Sum: 0.2172\n",
      "Step 22500/200000 MLoss: 0.0 GLoss: 0.2171 Sum: 0.2171\n",
      "Step 23000/200000 MLoss: 0.0 GLoss: 0.2156 Sum: 0.2156\n",
      "Step 23500/200000 MLoss: 0.0 GLoss: 0.2174 Sum: 0.2174\n",
      "Step 24000/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 24500/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 25000/200000 MLoss: 0.0 GLoss: 0.2158 Sum: 0.2158\n",
      "Step 25500/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 26000/200000 MLoss: 0.0 GLoss: 0.216 Sum: 0.216\n",
      "Step 26500/200000 MLoss: 0.0 GLoss: 0.2157 Sum: 0.2157\n",
      "Step 27000/200000 MLoss: 0.0 GLoss: 0.216 Sum: 0.216\n",
      "Step 27500/200000 MLoss: 0.0 GLoss: 0.2162 Sum: 0.2162\n",
      "Step 28000/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 28500/200000 MLoss: 0.0 GLoss: 0.217 Sum: 0.217\n",
      "Step 29000/200000 MLoss: 0.0 GLoss: 0.2152 Sum: 0.2152\n",
      "Step 29500/200000 MLoss: 0.0 GLoss: 0.2152 Sum: 0.2152\n",
      "Step 30000/200000 MLoss: 0.0 GLoss: 0.2146 Sum: 0.2146\n",
      "Step 30500/200000 MLoss: 0.0 GLoss: 0.2152 Sum: 0.2152\n",
      "Step 31000/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 31500/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 32000/200000 MLoss: 0.0 GLoss: 0.215 Sum: 0.215\n",
      "Step 32500/200000 MLoss: 0.0 GLoss: 0.2146 Sum: 0.2146\n",
      "Step 33000/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 33500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 34000/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 34500/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 35000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 35500/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 36000/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 36500/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 37000/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 37500/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 38000/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 38500/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 39000/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 39500/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 40000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 40500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 41000/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 41500/200000 MLoss: 0.0 GLoss: 0.2127 Sum: 0.2127\n",
      "Step 42000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 42500/200000 MLoss: 0.0 GLoss: 0.2127 Sum: 0.2127\n",
      "Step 43000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 43500/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 44000/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 44500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 45000/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 45500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 46000/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 46500/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 47000/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 47500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 48000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 48500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 49000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 49500/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 50000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 50500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 51000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 51500/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 52000/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 52500/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 53000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 53500/200000 MLoss: 0.0 GLoss: 0.2112 Sum: 0.2112\n",
      "Step 54000/200000 MLoss: 0.0 GLoss: 0.2112 Sum: 0.2112\n",
      "Step 54500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 55000/200000 MLoss: 0.0 GLoss: 0.2109 Sum: 0.2109\n",
      "Step 55500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 56000/200000 MLoss: 0.0 GLoss: 0.2105 Sum: 0.2105\n",
      "Step 56500/200000 MLoss: 0.0 GLoss: 0.2113 Sum: 0.2113\n",
      "Step 57000/200000 MLoss: 0.0 GLoss: 0.21 Sum: 0.21\n",
      "Step 57500/200000 MLoss: 0.0 GLoss: 0.2107 Sum: 0.2107\n",
      "Step 58000/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 58500/200000 MLoss: 0.0 GLoss: 0.211 Sum: 0.211\n",
      "Step 59000/200000 MLoss: 0.0 GLoss: 0.21 Sum: 0.21\n",
      "Step 59500/200000 MLoss: 0.0 GLoss: 0.2104 Sum: 0.2104\n",
      "Step 60000/200000 MLoss: 0.0 GLoss: 0.21 Sum: 0.21\n",
      "Step 60500/200000 MLoss: 0.0 GLoss: 0.2104 Sum: 0.2104\n",
      "Step 61000/200000 MLoss: 0.0 GLoss: 0.2106 Sum: 0.2106\n",
      "Step 61500/200000 MLoss: 0.0 GLoss: 0.2106 Sum: 0.2106\n",
      "Step 62000/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 62500/200000 MLoss: 0.0 GLoss: 0.2098 Sum: 0.2098\n",
      "Step 63000/200000 MLoss: 0.0 GLoss: 0.2106 Sum: 0.2106\n",
      "Step 63500/200000 MLoss: 0.0 GLoss: 0.2106 Sum: 0.2106\n",
      "Step 64000/200000 MLoss: 0.0 GLoss: 0.2105 Sum: 0.2105\n",
      "Step 64500/200000 MLoss: 0.0 GLoss: 0.2101 Sum: 0.2101\n",
      "Step 65000/200000 MLoss: 0.0 GLoss: 0.21 Sum: 0.21\n",
      "Step 65500/200000 MLoss: 0.0 GLoss: 0.2103 Sum: 0.2103\n",
      "Step 66000/200000 MLoss: 0.0 GLoss: 0.2088 Sum: 0.2088\n",
      "Step 66500/200000 MLoss: 0.0 GLoss: 0.2098 Sum: 0.2098\n",
      "Step 67000/200000 MLoss: 0.0 GLoss: 0.2101 Sum: 0.2101\n",
      "Step 67500/200000 MLoss: 0.0 GLoss: 0.2097 Sum: 0.2097\n",
      "Step 68000/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 68500/200000 MLoss: 0.0 GLoss: 0.2085 Sum: 0.2085\n",
      "Step 69000/200000 MLoss: 0.0 GLoss: 0.2096 Sum: 0.2096\n",
      "Step 69500/200000 MLoss: 0.0 GLoss: 0.2094 Sum: 0.2094\n",
      "Step 70000/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 70500/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 71000/200000 MLoss: 0.0 GLoss: 0.2087 Sum: 0.2087\n",
      "Step 71500/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 72000/200000 MLoss: 0.0 GLoss: 0.209 Sum: 0.209\n",
      "Step 72500/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 73000/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 73500/200000 MLoss: 0.0 GLoss: 0.2086 Sum: 0.2086\n",
      "Step 74000/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 74500/200000 MLoss: 0.0 GLoss: 0.2095 Sum: 0.2095\n",
      "Step 75000/200000 MLoss: 0.0 GLoss: 0.2087 Sum: 0.2087\n",
      "Step 75500/200000 MLoss: 0.0 GLoss: 0.2095 Sum: 0.2095\n",
      "Step 76000/200000 MLoss: 0.0 GLoss: 0.2089 Sum: 0.2089\n",
      "Step 76500/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 77000/200000 MLoss: 0.0 GLoss: 0.2082 Sum: 0.2082\n",
      "Step 77500/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 78000/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 78500/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 79000/200000 MLoss: 0.0 GLoss: 0.2093 Sum: 0.2093\n",
      "Step 79500/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 80000/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 80500/200000 MLoss: 0.0 GLoss: 0.2098 Sum: 0.2098\n",
      "Step 81000/200000 MLoss: 0.0 GLoss: 0.2086 Sum: 0.2086\n",
      "Step 81500/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 82000/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 82500/200000 MLoss: 0.0 GLoss: 0.2084 Sum: 0.2084\n",
      "Step 83000/200000 MLoss: 0.0 GLoss: 0.208 Sum: 0.208\n",
      "Step 83500/200000 MLoss: 0.0 GLoss: 0.2083 Sum: 0.2083\n",
      "Step 84000/200000 MLoss: 0.0 GLoss: 0.2084 Sum: 0.2084\n",
      "Step 84500/200000 MLoss: 0.0 GLoss: 0.2088 Sum: 0.2088\n",
      "Step 85000/200000 MLoss: 0.0 GLoss: 0.2086 Sum: 0.2086\n",
      "Step 85500/200000 MLoss: 0.0 GLoss: 0.2087 Sum: 0.2087\n",
      "Step 86000/200000 MLoss: 0.0 GLoss: 0.2081 Sum: 0.2081\n",
      "Step 86500/200000 MLoss: 0.0 GLoss: 0.2083 Sum: 0.2083\n",
      "Step 87000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 87500/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 88000/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 88500/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 89000/200000 MLoss: 0.0 GLoss: 0.2078 Sum: 0.2078\n",
      "Step 89500/200000 MLoss: 0.0 GLoss: 0.2082 Sum: 0.2082\n",
      "Step 90000/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 90500/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 91000/200000 MLoss: 0.0 GLoss: 0.2069 Sum: 0.2069\n",
      "Step 91500/200000 MLoss: 0.0 GLoss: 0.2075 Sum: 0.2075\n",
      "Step 92000/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 92500/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 93000/200000 MLoss: 0.0 GLoss: 0.2087 Sum: 0.2087\n",
      "Step 93500/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 94000/200000 MLoss: 0.0 GLoss: 0.2078 Sum: 0.2078\n",
      "Step 94500/200000 MLoss: 0.0 GLoss: 0.207 Sum: 0.207\n",
      "Step 95000/200000 MLoss: 0.0 GLoss: 0.2071 Sum: 0.2071\n",
      "Step 95500/200000 MLoss: 0.0 GLoss: 0.2071 Sum: 0.2071\n",
      "Step 96000/200000 MLoss: 0.0 GLoss: 0.2071 Sum: 0.2071\n",
      "Step 96500/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 97000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 97500/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 98000/200000 MLoss: 0.0 GLoss: 0.2078 Sum: 0.2078\n",
      "Step 98500/200000 MLoss: 0.0 GLoss: 0.2071 Sum: 0.2071\n",
      "Step 99000/200000 MLoss: 0.0 GLoss: 0.2066 Sum: 0.2066\n",
      "Step 99500/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 100000/200000 MLoss: 0.0 GLoss: 0.2075 Sum: 0.2075\n",
      "Step 100500/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 101000/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 101500/200000 MLoss: 0.0 GLoss: 0.2077 Sum: 0.2077\n",
      "Step 102000/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 102500/200000 MLoss: 0.0 GLoss: 0.2061 Sum: 0.2061\n",
      "Step 103000/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 103500/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 104000/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 104500/200000 MLoss: 0.0 GLoss: 0.2062 Sum: 0.2062\n",
      "Step 105000/200000 MLoss: 0.0 GLoss: 0.2064 Sum: 0.2064\n",
      "Step 105500/200000 MLoss: 0.0 GLoss: 0.2069 Sum: 0.2069\n",
      "Step 106000/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 106500/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 107000/200000 MLoss: 0.0 GLoss: 0.2057 Sum: 0.2057\n",
      "Step 107500/200000 MLoss: 0.0 GLoss: 0.2064 Sum: 0.2064\n",
      "Step 108000/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 108500/200000 MLoss: 0.0 GLoss: 0.2061 Sum: 0.2061\n",
      "Step 109000/200000 MLoss: 0.0 GLoss: 0.2069 Sum: 0.2069\n",
      "Step 109500/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 110000/200000 MLoss: 0.0 GLoss: 0.2062 Sum: 0.2062\n",
      "Step 110500/200000 MLoss: 0.0 GLoss: 0.2064 Sum: 0.2064\n",
      "Step 111000/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 111500/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 112000/200000 MLoss: 0.0 GLoss: 0.2066 Sum: 0.2066\n",
      "Step 112500/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 113000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 113500/200000 MLoss: 0.0 GLoss: 0.2058 Sum: 0.2058\n",
      "Step 114000/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 114500/200000 MLoss: 0.0 GLoss: 0.2064 Sum: 0.2064\n",
      "Step 115000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 115500/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 116000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 116500/200000 MLoss: 0.0 GLoss: 0.2061 Sum: 0.2061\n",
      "Step 117000/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 117500/200000 MLoss: 0.0 GLoss: 0.2061 Sum: 0.2061\n",
      "Step 118000/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 118500/200000 MLoss: 0.0 GLoss: 0.2061 Sum: 0.2061\n",
      "Step 119000/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 119500/200000 MLoss: 0.0 GLoss: 0.2054 Sum: 0.2054\n",
      "Step 120000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 120500/200000 MLoss: 0.0 GLoss: 0.2058 Sum: 0.2058\n",
      "Step 121000/200000 MLoss: 0.0 GLoss: 0.2057 Sum: 0.2057\n",
      "Step 121500/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 122000/200000 MLoss: 0.0 GLoss: 0.2058 Sum: 0.2058\n",
      "Step 122500/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 123000/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 123500/200000 MLoss: 0.0 GLoss: 0.2058 Sum: 0.2058\n",
      "Step 124000/200000 MLoss: 0.0 GLoss: 0.2054 Sum: 0.2054\n",
      "Step 124500/200000 MLoss: 0.0 GLoss: 0.2058 Sum: 0.2058\n",
      "Step 125000/200000 MLoss: 0.0 GLoss: 0.2055 Sum: 0.2055\n",
      "Step 125500/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 126000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 126500/200000 MLoss: 0.0 GLoss: 0.2049 Sum: 0.2049\n",
      "Step 127000/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 127500/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 128000/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 128500/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 129000/200000 MLoss: 0.0 GLoss: 0.2057 Sum: 0.2057\n",
      "Step 129500/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 130000/200000 MLoss: 0.0 GLoss: 0.2057 Sum: 0.2057\n",
      "Step 130500/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 131000/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 131500/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 132000/200000 MLoss: 0.0 GLoss: 0.204 Sum: 0.204\n",
      "Step 132500/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 133000/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 133500/200000 MLoss: 0.0 GLoss: 0.2048 Sum: 0.2048\n",
      "Step 134000/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 134500/200000 MLoss: 0.0 GLoss: 0.2046 Sum: 0.2046\n",
      "Step 135000/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 135500/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 136000/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 136500/200000 MLoss: 0.0 GLoss: 0.2042 Sum: 0.2042\n",
      "Step 137000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 137500/200000 MLoss: 0.0 GLoss: 0.2054 Sum: 0.2054\n",
      "Step 138000/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 138500/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 139000/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 139500/200000 MLoss: 0.0 GLoss: 0.2048 Sum: 0.2048\n",
      "Step 140000/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 140500/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 141000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 141500/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 142000/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 142500/200000 MLoss: 0.0 GLoss: 0.2048 Sum: 0.2048\n",
      "Step 143000/200000 MLoss: 0.0 GLoss: 0.2042 Sum: 0.2042\n",
      "Step 143500/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 144000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 144500/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 145000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 145500/200000 MLoss: 0.0 GLoss: 0.2042 Sum: 0.2042\n",
      "Step 146000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 146500/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 147000/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 147500/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 148000/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 148500/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 149000/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 149500/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 150000/200000 MLoss: 0.0 GLoss: 0.2044 Sum: 0.2044\n",
      "Step 150500/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 151000/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 151500/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 152000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 152500/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 153000/200000 MLoss: 0.0 GLoss: 0.2048 Sum: 0.2048\n",
      "Step 153500/200000 MLoss: 0.0 GLoss: 0.2032 Sum: 0.2032\n",
      "Step 154000/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 154500/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 155000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 155500/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 156000/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 156500/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 157000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 157500/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 158000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 158500/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 159000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 159500/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 160000/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 160500/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 161000/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 161500/200000 MLoss: 0.0 GLoss: 0.2032 Sum: 0.2032\n",
      "Step 162000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 162500/200000 MLoss: 0.0 GLoss: 0.2031 Sum: 0.2031\n",
      "Step 163000/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 163500/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 164000/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 164500/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 165000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 165500/200000 MLoss: 0.0 GLoss: 0.2023 Sum: 0.2023\n",
      "Step 166000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 166500/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 167000/200000 MLoss: 0.0 GLoss: 0.2031 Sum: 0.2031\n",
      "Step 167500/200000 MLoss: 0.0 GLoss: 0.2026 Sum: 0.2026\n",
      "Step 168000/200000 MLoss: 0.0 GLoss: 0.2031 Sum: 0.2031\n",
      "Step 168500/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 169000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 169500/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 170000/200000 MLoss: 0.0 GLoss: 0.2032 Sum: 0.2032\n",
      "Step 170500/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 171000/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 171500/200000 MLoss: 0.0 GLoss: 0.2031 Sum: 0.2031\n",
      "Step 172000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 172500/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 173000/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 173500/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 174000/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 174500/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 175000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 175500/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 176000/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 176500/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 177000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 177500/200000 MLoss: 0.0 GLoss: 0.2023 Sum: 0.2023\n",
      "Step 178000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 178500/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 179000/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 179500/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 180000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 180500/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 181000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 181500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 182000/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 182500/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 183000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 183500/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 184000/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 184500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 185000/200000 MLoss: 0.0 GLoss: 0.2022 Sum: 0.2022\n",
      "Step 185500/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 186000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 186500/200000 MLoss: 0.0 GLoss: 0.202 Sum: 0.202\n",
      "Step 187000/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 187500/200000 MLoss: 0.0 GLoss: 0.2015 Sum: 0.2015\n",
      "Step 188000/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 188500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 189000/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 189500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 190000/200000 MLoss: 0.0 GLoss: 0.2014 Sum: 0.2014\n",
      "Step 190500/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 191000/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 191500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 192000/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 192500/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 193000/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 193500/200000 MLoss: 0.0 GLoss: 0.2022 Sum: 0.2022\n",
      "Step 194000/200000 MLoss: 0.0 GLoss: 0.2008 Sum: 0.2008\n",
      "Step 194500/200000 MLoss: 0.0 GLoss: 0.2015 Sum: 0.2015\n",
      "Step 195000/200000 MLoss: 0.0 GLoss: 0.2015 Sum: 0.2015\n",
      "Step 195500/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 196000/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 196500/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 197000/200000 MLoss: 0.0 GLoss: 0.2011 Sum: 0.2011\n",
      "Step 197500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 198000/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 198500/200000 MLoss: 0.0 GLoss: 0.2011 Sum: 0.2011\n",
      "Step 199000/200000 MLoss: 0.0 GLoss: 0.2015 Sum: 0.2015\n",
      "Step 199500/200000 MLoss: 0.0 GLoss: 0.2014 Sum: 0.2014\n",
      "Step 200000/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Training time: 14163.275846481323 seconds\n",
      "==================== We show the important sampling parameters below ====================\n",
      "batch_size: 20000\n",
      "classifier_scale: 1.0\n",
      "\n",
      "Generating None -> trans\n",
      "Sample size: 20000\n",
      "Sample timestep  345\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table\n",
    "\n",
    "AUX_DIRS= ['/home/hadrien/Documents/Phd/ensemble-mia/data/tabddpm_black_box/train','/home/hadrien/Documents/Phd/ensemble-mia/data/tabsyn_black_box/train']\n",
    "\n",
    "\n",
    "def deduplicate_transactions(df):\n",
    "    return df.drop_duplicates(subset=['trans_id','account_id'])\n",
    "def load_aux_dataframes(train_dir):\n",
    "    # Load all train_with_id CSV files from train directory\n",
    "    train_dfs = []\n",
    "    for train_dir in AUX_DIRS:\n",
    "        for sub_folder in os.listdir(train_dir):\n",
    "            file_path = os.path.join(train_dir, sub_folder, 'train_with_id.csv')\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_dfs.append(df)\n",
    "    \n",
    "    return deduplicate_transactions(pd.concat(train_dfs, ignore_index=True))\n",
    "\n",
    "full_table = load_aux_dataframes(AUX_DIRS)\n",
    "\n",
    "MODEL_NAME= 'tabddpm' #'tabsyn'\n",
    "TRAIN_DIR= f'/home/hadrien/Documents/Phd/ensemble-mia/data/{MODEL_NAME}_black_box/train'\n",
    "CHALLENGE_DIR_DEV= f'/home/hadrien/Documents/Phd/ensemble-mia/data/{MODEL_NAME}_black_box/dev'\n",
    "CHALLENGE_DIR_FINAL= f'/home/hadrien/Documents/Phd/ensemble-mia/data/{MODEL_NAME}_black_box/final'\n",
    "\n",
    "all_challenge_dirs= [CHALLENGE_DIR_DEV, CHALLENGE_DIR_FINAL]#, TRAIN_DIR]\n",
    "\n",
    "all_challenge_tables= []\n",
    "\n",
    "for challenge_dir in all_challenge_dirs:\n",
    "    for sub_folder in os.listdir(challenge_dir):\n",
    "        file_path = os.path.join(challenge_dir, sub_folder, 'challenge_with_id.csv')\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_challenge_tables.append(df)\n",
    "\n",
    "all_challenge= pd.concat(all_challenge_tables, axis=0)\n",
    "# Drop all observations from full_table that appear in all_challenge based on account_id and trans_id\n",
    "full_table = full_table.drop_duplicates(subset=['account_id', 'trans_id'])\n",
    "print(full_table.shape)\n",
    "full_table = full_table[~full_table.set_index(['account_id', 'trans_id']).index.isin(\n",
    "    all_challenge.set_index(['account_id', 'trans_id']).index\n",
    ")]\n",
    "print(full_table.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_table.to_csv(os.path.join('/home/hadrien/Documents/Phd/ensemble-mia/data/shadow_ref','full_table.csv'), index=False)\n",
    "all_challenge.to_csv(os.path.join('/home/hadrien/Documents/Phd/ensemble-mia/data/shadow_ref','all_challenge_tables.csv'), index=False)\n",
    "\n",
    "config_file_path= 'midst_models/single_table_TabDDPM/configs'\n",
    "\n",
    "\n",
    "for i in range(3,11):\n",
    "    workspace= f'{TRAIN_DIR}/tabddpm_20{i}'\n",
    "    # Create the new folder if it doesn't exist\n",
    "    os.makedirs(workspace, exist_ok=True)\n",
    "\n",
    "    df_all_challenge= pd.read_csv(os.path.join('/home/hadrien/Documents/Phd/ensemble-mia/data/shadow_ref','all_challenge_tables.csv'))\n",
    "    full_table= pd.read_csv(os.path.join('/home/hadrien/Documents/Phd/ensemble-mia/data/shadow_ref','full_table.csv'))\n",
    "\n",
    "    challenge_sample= df_all_challenge.sample(n=2   000, replace=False)\n",
    "    aux_data= full_table.sample(n=18000, replace=False)\n",
    "\n",
    "    df_train= pd.concat([challenge_sample, aux_data], axis=0)\n",
    "\n",
    "    df_train.to_csv(os.path.join(workspace,'train_with_id.csv'), index=False)\n",
    "    df_train.drop(columns=['account_id', 'trans_id']).to_csv(os.path.join(workspace,'train.csv'), index=False)\n",
    "\n",
    "\n",
    "    # Copy the original config file to the new folder\n",
    "    shutil.copy(os.path.join(config_file_path, \"trans.json\"), workspace)\n",
    "    shutil.copy(os.path.join(config_file_path, \"dataset_meta.json\"), workspace)\n",
    "    shutil.copy(os.path.join(config_file_path, \"trans_domain.json\"), workspace)\n",
    "\n",
    "    # Modify the config file\n",
    "    with open(os.path.join(workspace, \"trans.json\"), \"r\") as file:\n",
    "        trans_config = json.load(file)\n",
    "\n",
    "    trans_config[\"general\"][\"data_dir\"] = str(workspace)\n",
    "    trans_config[\"general\"][\"workspace_dir\"] = str(workspace)\n",
    "    trans_config[\"general\"][\"test_data_dir\"] = \"\"\n",
    "\n",
    "\n",
    "    # Save the changed\n",
    "    with open(os.path.join(workspace, \"trans.json\"), \"w\") as file:\n",
    "        json.dump(trans_config, file, indent=4)\n",
    "\n",
    "    # Load config\n",
    "    config_path = f\"{workspace}/trans.json\"\n",
    "    configs, save_dir = load_configs(config_path)\n",
    "\n",
    "    # Display config\n",
    "    json_str = json.dumps(configs, indent=4)\n",
    "    print(json_str)\n",
    "\n",
    "    # Load  dataset\n",
    "    # In this step, we load the dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "    tables, relation_order, dataset_meta = load_multi_table(configs[\"general\"][\"data_dir\"])\n",
    "    print(\"\")\n",
    "\n",
    "    # Tables is a dictionary of the multi-table dataset\n",
    "    print(\n",
    "        \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    "    )\n",
    "    print(list(tables.keys()))\n",
    "\n",
    "    # Display important clustering parameters\n",
    "    params_clustering = configs[\"clustering\"]\n",
    "    print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "    for key, val in params_clustering.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Clustering on the multi-table dataset\n",
    "    tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "        tables, relation_order, save_dir, configs\n",
    "    )\n",
    "\n",
    "    # Display important sampling parameters\n",
    "    params_sampling = configs[\"diffusion\"]\n",
    "    print(\n",
    "        \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    "    )\n",
    "    for key, val in params_sampling.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Launch training from scratch\n",
    "\n",
    "    t= time.time()\n",
    "    models = clava_training(tables, relation_order, save_dir, configs)\n",
    "    print(f\"Training time: {time.time() - t} seconds\")\n",
    "\n",
    "    # Display important sampling parameters\n",
    "    params_sampling = configs[\"sampling\"]\n",
    "    print(\n",
    "        \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    "    )\n",
    "    for key, val in params_sampling.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "    print(\"\")\n",
    "\n",
    "    cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "        tables,\n",
    "        relation_order,\n",
    "        save_dir,\n",
    "        all_group_lengths_prob_dicts,\n",
    "        models,\n",
    "        configs,\n",
    "        sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    "    )\n",
    "\n",
    "    # Cast int values that saved as string to int for further evaluation\n",
    "    for key in cleaned_tables.keys():\n",
    "        for col in cleaned_tables[key].columns:\n",
    "            if cleaned_tables[key][col].dtype == \"object\":\n",
    "                try:\n",
    "                    cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "                except ValueError:\n",
    "                    print(f\"Column {col} cannot be converted to int.\")\n",
    "    # ajouter une commande pour copier les donnes synth dans le bon dossier\n",
    "    #shutil.copy(os.path.join(save_dir, \"trans\",\"final\",\"trans_synthetic.csv\"), os.path.join(workspace, \"trans_synthetic.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midst_models",
   "language": "python",
   "name": "midst_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
